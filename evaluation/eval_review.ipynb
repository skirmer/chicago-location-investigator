{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325bcc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "file_list = [f for f in os.listdir('../data') if f.endswith('.json')]\n",
    "\n",
    "def format_data(file_list):\n",
    "    results_list = []\n",
    "    for i in file_list:\n",
    "        with open(f\"../data/{i}\", 'r') as f:\n",
    "            data = json.load(f)\n",
    "           \n",
    "            data_formatted = {\"timestamp\": data['timestamp'],\n",
    "                            \"prompt\": data['prompt'],\n",
    "                            \"expected_output\": data[\"expected_output\"],\n",
    "                            \"expected_tool_names\": data['expected_tool_names'],\n",
    "                            \"metrics\": data['metrics']}\n",
    "\n",
    "            results_list.append(data_formatted)\n",
    "            \n",
    "    return pd.DataFrame(results_list)\n",
    "\n",
    "df = format_data(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f743e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd1111",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    df[f'metrics_{i}'] = df['metrics'].apply(lambda x: x[i])\n",
    "    df[f'metrics_{i}_name'] = df[f'metrics_{i}'].apply(lambda x: x['name'])\n",
    "    df[f'metrics_{i}_score'] = df[f'metrics_{i}'].apply(lambda x: x['score'])\n",
    "    df[f'metrics_{i}_success'] = df[f'metrics_{i}'].apply(lambda x: x['success'])\n",
    "    df[f'metrics_{i}_reason'] = df[f'metrics_{i}'].apply(lambda x: x['reason'])\n",
    "\n",
    "clean_df = df[['timestamp', 'prompt','expected_output','expected_tool_names',\n",
    "    'metrics_0_name', 'metrics_0_score', 'metrics_0_success', 'metrics_0_reason',\n",
    "    'metrics_1_name', 'metrics_1_score', 'metrics_1_success', 'metrics_1_reason',\n",
    "    'metrics_2_name', 'metrics_2_score', 'metrics_2_success', 'metrics_2_reason']]\n",
    "\n",
    "clean_df['prompt_trunc'] = clean_df['prompt'].str[:30]\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693e7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "metric_num = 2\n",
    "\n",
    "\n",
    "charts = []\n",
    "for metric_num in [0,1,2]:\n",
    "\n",
    "    charts.append(alt.Chart(clean_df).mark_line().encode(\n",
    "        x='timestamp',\n",
    "        y=f'metrics_{metric_num}_score',\n",
    "        color='prompt',\n",
    "        facet=alt.Facet('prompt_trunc', columns=2)\n",
    "    ).properties(title = f\"Performance over Time, Metric: {clean_df[f'metrics_{metric_num}_name'].iloc[0]}\", width=400)\n",
    "    )\n",
    "\n",
    "charts[0] & charts[1] & charts[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58687d1",
   "metadata": {},
   "source": [
    "# Failed Tests Drilldown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95112c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_df.head()\n",
    "from IPython.display import Markdown\n",
    "\n",
    "for idx, row in clean_df.iterrows():\n",
    "\n",
    "    if row['metrics_0_success'] is False:\n",
    "        md = f\"\"\"\n",
    "    Metric: {row['metrics_0_name']}\n",
    "    Prompt: {row['prompt']} \n",
    "    \n",
    "    Failure Reason: {row['metrics_0_reason']}\n",
    "        \"\"\"\n",
    "        display(Markdown(md))\n",
    "        \n",
    "    if row['metrics_1_success'] is False:\n",
    "        md = f\"\"\"\n",
    "    Metric: {row['metrics_1_name']}\n",
    "    Prompt: {row['prompt']} \n",
    "    \n",
    "    Failure Reason: {row['metrics_1_reason']}\n",
    "        \"\"\"\n",
    "        display(Markdown(md))\n",
    "        \n",
    "    if row['metrics_2_success'] is False:\n",
    "        md = f\"\"\"\n",
    "    Metric: {row['metrics_2_name']}\n",
    "    Prompt: {row['prompt']} \n",
    "    \n",
    "    Failure Reason: {row['metrics_2_reason']}\n",
    "        \"\"\"\n",
    "        display(Markdown(md))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chi_loc_investigator",
   "language": "python",
   "name": "chi_loc_investigator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
